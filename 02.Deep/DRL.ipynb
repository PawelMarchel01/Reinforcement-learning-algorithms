{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vqgCBtjBUpix",
   "metadata": {
    "id": "vqgCBtjBUpix"
   },
   "source": [
    "# Deep Reinforcement Learning (DRL) and Weight Agnostic Networks (WAN) on the Pendulum Environment\n",
    "\n",
    "In this notebook, we will explore two different approaches to controlling an environment with continuous actions:\n",
    "\n",
    "1. **Deep Reinforcement Learning (DRL)** — where an agent learns a policy using a neural network that is trained to maximize cumulative rewards through interaction with the environment.\n",
    "\n",
    "2. **Weight Agnostic Neural Networks (WAN)** — a method that uses fixed network architectures and weights, relying on the structure of the network and activation functions rather than extensive training.\n",
    "\n",
    "Our test environment will be the classic **Pendulum** problem, where the goal is to balance a pendulum upright by applying continuous torque.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4414e1-505d-47b5-af57-27e56c6793f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from game import PendulumEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vA2tF2loS1dB",
   "metadata": {
    "id": "vA2tF2loS1dB"
   },
   "source": [
    "# Deep Reinforcement Learning (DRL)\n",
    "\n",
    "This class implements a simple Deep Reinforcement Learning (DRL) agent using a neural network built with PyTorch. The agent learns how to choose actions based on the current state of the environment to maximize future rewards.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Neural Network (`self.mapping`)**  \n",
    "  A feedforward network with two hidden layers using ReLU activation functions. It takes the current state as input and outputs predicted action values (Q-values).\n",
    "\n",
    "- **Weights Initialization**  \n",
    "  The linear layers’ weights are initialized with a normal distribution (mean = 0, std = 0.1), and biases are set to zero to help the training start smoothly.\n",
    "\n",
    "- **Epsilon-Greedy Exploration (`explore` method)**  \n",
    "  To balance exploring new actions and using what it has learned, the agent picks actions using an epsilon-greedy method:  \n",
    "  - With a probability `epsilon` (which decreases over time), it selects a **random action** to explore new possibilities.  \n",
    "  - Otherwise, it selects the **best action** predicted by the neural network.\n",
    "\n",
    "- **Experience Replay Buffer (`remember` method)**  \n",
    "  The agent stores its experiences — tuples of `(state, action, reward, next_state)` — in a fixed-size buffer. This helps the agent learn from a diverse set of past experiences and reduces the problem of learning from highly correlated data.\n",
    "\n",
    "- **Training Step (`rethink` method)**  \n",
    "  When enough experiences are collected, the agent samples a random batch from the buffer and updates the neural network by minimizing the difference between predicted and target Q-values:  \n",
    "  - **Target Q-values (`y_true`):** Calculated as the immediate reward plus the discounted best future reward predicted from the next state (`gamma` is the discount factor).  \n",
    "  - **Predicted Q-values (`y_pred`):** The neural network’s output for the sampled states.  \n",
    "  The agent uses Mean Squared Error loss and the Adam optimizer to update the network weights.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**  \n",
    "This DRL agent learns how to map states to actions by interacting with the environment, exploring with some randomness, and improving its decisions based on past experiences. The epsilon-greedy approach balances trying new actions and using known good ones, while experience replay and neural network training steadily improve the agent’s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc5e5b8-38d1-4347-9720-816d69a770fa",
   "metadata": {
    "id": "4cc5e5b8-38d1-4347-9720-816d69a770fa"
   },
   "outputs": [],
   "source": [
    "class DRL(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
    "        super(DRL, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.mapping = torch.nn.Sequential(torch.nn.Linear(input_size, hidden_size), torch.nn.ReLU(), torch.nn.Linear(hidden_size, hidden_size), torch.nn.ReLU(), torch.nn.Linear(hidden_size, output_size))\n",
    "        self.apply(self.__class__.weights_init)  #TPJ\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.steps = 0\n",
    "        self.buffer = []\n",
    "        self.epsi_low = 0.05\n",
    "        self.epsi_high = 0.9\n",
    "        self.gamma = 0.8\n",
    "        self.decay = 200\n",
    "        self.capacity = 10000\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def weights_init(m):\n",
    "        if m.__class__.__name__.find('Linear') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.1)\n",
    "            torch.nn.init.constant_(m.bias.data, val=0.0)\n",
    "\n",
    "    def explore(self, state):\n",
    "        self.steps += 1\n",
    "        epsilon = self.epsi_low + (self.epsi_high - self.epsi_low) * math.exp(-1.0 * self.steps / self.decay)\n",
    "        if random.random() < epsilon:\n",
    "            # Explore: Select a random action in the range (-2.0, 2.0)\n",
    "            action = random.uniform(-2.0, 2.0)\n",
    "        else:\n",
    "            # Exploitation: Continuous Action Forecast\n",
    "            state = torch.tensor(state, dtype=torch.float).view(1, -1)\n",
    "            action = self.mapping(state).item()\n",
    "        return action\n",
    "\n",
    "    def remember(self, *transition):\n",
    "        if len( self.buffer)==self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def rethink(self):\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "            state_old, action_now, reward_now, state_new = zip(*random.sample(self.buffer, self.batch_size))\n",
    "\n",
    "            state_old = torch.tensor(np.array(state_old), dtype=torch.float)\n",
    "            action_now = torch.tensor(np.array(action_now), dtype=torch.float).view(self.batch_size, -1)\n",
    "            reward_now = torch.tensor(np.array(reward_now), dtype=torch.float).view(self.batch_size, -1)\n",
    "            state_new = torch.tensor(np.array(state_new), dtype=torch.float)\n",
    "\n",
    "            y_true = reward_now + self.gamma * torch.max(self.mapping(state_new).detach(), dim=1)[0].view(self.batch_size, -1)\n",
    "\n",
    "            y_pred = self.mapping(state_old)\n",
    "\n",
    "            loss = self.criterion(y_pred, y_true)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46657757-8eb8-40e7-9501-5554426de40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drl(environment):\n",
    "    drl = DRL(environment.observation_space, 256, 1, learning_rate=0.001)\n",
    "    exploration_steps = 1000\n",
    "    for epoch in range(10):\n",
    "        state_old = environment.reset()\n",
    "        rewards = 0\n",
    "        max_steps = 2000\n",
    "        steps = 0\n",
    "\n",
    "        while steps < max_steps:\n",
    "            steps += 1\n",
    "            environment.render()\n",
    "\n",
    "            if len(drl.buffer) < exploration_steps:\n",
    "                action_now = random.uniform(*environment.action_space)\n",
    "            else:\n",
    "                action_now = drl.explore(state_old)\n",
    "\n",
    "            state_new, reward_now, done, _ = environment.step(action_now)\n",
    "            drl.remember(state_old, action_now, reward_now, state_new)\n",
    "            rewards += reward_now\n",
    "            state_old = state_new\n",
    "\n",
    "            if len(drl.buffer) >= drl.batch_size:\n",
    "                drl.rethink()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f'epoch={epoch:04d}, rewards={rewards:.2f}, step={steps}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e0dde5-0015-4ac6-b8b7-3d30a2b3d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRL\n",
      "epoch=0000, rewards=-11812.68, step=1067\n",
      "epoch=0001, rewards=-10494.94, step=312\n",
      "epoch=0002, rewards=-638.35, step=1241\n",
      "epoch=0003, rewards=-10174.40, step=86\n",
      "epoch=0004, rewards=-200.50, step=736\n",
      "epoch=0005, rewards=-918.91, step=1435\n",
      "epoch=0006, rewards=-86.76, step=267\n",
      "epoch=0007, rewards=-260.19, step=506\n",
      "epoch=0008, rewards=-623.31, step=618\n",
      "epoch=0009, rewards=-10179.22, step=86\n"
     ]
    }
   ],
   "source": [
    "env = PendulumEnv()\n",
    "print(\"DRL\")\n",
    "drl(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wDXM9HJTT7Dl",
   "metadata": {
    "id": "wDXM9HJTT7Dl"
   },
   "source": [
    "## Summary\n",
    "\n",
    "- **Setup:**  \n",
    "  The DRL agent uses a neural network with two hidden layers to predict actions based on the current pendulum state. It learns through interaction with the environment by balancing exploration and exploitation.\n",
    "\n",
    "- **Training Process:**  \n",
    "  - Initially performs random actions to collect experiences (exploration phase).  \n",
    "  - Uses an experience replay buffer to sample past transitions and train the network.  \n",
    "  - Employs an epsilon-greedy strategy where random actions decrease over time.  \n",
    "\n",
    "- **Results:**  \n",
    "  - Rewards vary significantly across epochs, sometimes very negative due to large penalties when the pendulum falls below a threshold.  \n",
    "  - The agent improves somewhat but training is noisy and unstable within 10 epochs.  \n",
    "  - Maximum steps reached in some episodes indicate partial success in controlling the pendulum."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
