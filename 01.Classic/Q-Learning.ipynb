{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39beb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from Pong import PongGame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd2757",
   "metadata": {},
   "source": [
    "# Q-Learning for Pong\n",
    "\n",
    "This notebook demonstrates how Q-learning — a model-free reinforcement learning algorithm — can be applied to train an agent to play a simplified version of Pong.\n",
    "\n",
    "## What is Q-learning?\n",
    "\n",
    "Q-learning is an off-policy, value-based reinforcement learning algorithm. Its goal is to learn the optimal action-selection policy that maximizes the expected cumulative reward over time.\n",
    "\n",
    "The agent learns a function Q(s, a), which estimates the value (expected future reward) of taking action `a` in state `s`, and then following the optimal policy. Over time, these Q-values are updated using the Bellman equation:\n",
    "\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- `α` is the learning rate (how much to update)\n",
    "- `γ` is the discount factor (how much future rewards matter)\n",
    "- `r` is the reward received after taking action `a` in state `s`\n",
    "- `s'` is the next state\n",
    "- `max_a' Q(s', a')` is the estimated value of the best action in the next state\n",
    "\n",
    "## How it's applied here\n",
    "\n",
    "In this project:\n",
    "- The game environment (Pong) is **discretized** — continuous variables like ball and paddle positions are binned into finite ranges.\n",
    "- The **state space** consists of: paddle Y position, ball X and Y position, and ball X and Y directions.\n",
    "- The **action space** is limited to three options: move up, stay, or move down.\n",
    "- A **Q-table** is used to store and update values for each (state, action) pair.\n",
    "\n",
    "The agent uses an **epsilon-greedy strategy** to explore:\n",
    "- With probability ε, it chooses a random action (exploration).\n",
    "- Otherwise, it selects the action with the highest Q-value (exploitation).\n",
    "\n",
    "During training:\n",
    "- The agent interacts with the environment for many episodes.\n",
    "- Rewards guide the paddle to track and intercept the ball.\n",
    "- Q-values are updated incrementally based on experience.\n",
    "\n",
    "At the end of training, the Q-table is converted into a deterministic policy by selecting the best action for each known state. This policy is saved to a file and can be used in the game for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dda2719-73dd-4bd2-ae2a-4469df49043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "EPSILON = 0.1\n",
    "EPISODES = 200000\n",
    "MAX_ACTIONS_PER_EPISODE = 2000\n",
    "\n",
    "NUM_PADDLE_POS = 30\n",
    "NUM_BALL_X = 50\n",
    "NUM_BALL_Y = 40\n",
    "\n",
    "ACTIONS = [-1, 0, 1] \n",
    "\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "PADDLE_WIDTH, PADDLE_HEIGHT = 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b20873c-cf1d-4f73-a851-dc31e5516b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(value, max_value, bins):\n",
    "    return min(bins - 1, max(0, int(value / max_value * bins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ffd18-74d1-4dd6-97c6-9c182e262966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(state, action):\n",
    "    paddle_y, ball_x, ball_y, ball_dx, ball_dy = state\n",
    "\n",
    "    paddle_center = paddle_y * (HEIGHT / NUM_PADDLE_POS) + PADDLE_HEIGHT / 2\n",
    "    ball_actual_y = ball_y * (HEIGHT / NUM_BALL_Y)\n",
    "\n",
    "    distance = abs(paddle_center - ball_actual_y)\n",
    "    reward = -distance / (HEIGHT / 2)\n",
    "\n",
    "    if (ball_actual_y > paddle_center and action == 1) or (ball_actual_y < paddle_center and action == -1):\n",
    "        reward += 0.5 \n",
    "\n",
    "    if ball_dx == 1 and ball_x == NUM_BALL_X - 1:\n",
    "        if distance <= PADDLE_HEIGHT / 2:\n",
    "            reward += 5  \n",
    "        else:\n",
    "            reward -= 100 \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569548c2-e2ea-4459-a3a5-da2e53deb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = {}\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if state not in q_table:\n",
    "        q_table[state] = [0] * len(ACTIONS)\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(q_table[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bf272-982c-4393-a865-8d3fc7c0e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state, action, reward, next_state):\n",
    "    if next_state not in q_table:\n",
    "        q_table[next_state] = [0] * len(ACTIONS)\n",
    "    action_index = ACTIONS.index(action)\n",
    "    best_next_action = max(q_table[next_state])\n",
    "    q_table[state][action_index] += LEARNING_RATE * (reward + DISCOUNT_FACTOR * best_next_action - q_table[state][action_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0736be63-ad83-44d7-bea1-2d915ffeee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25000/200000\n",
      "Episode 50000/200000\n",
      "Episode 75000/200000\n",
      "Episode 100000/200000\n",
      "Episode 125000/200000\n",
      "Episode 150000/200000\n",
      "Episode 175000/200000\n",
      "Episode 200000/200000\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES):\n",
    "\n",
    "    paddle_y = random.randint(0, NUM_PADDLE_POS - 1)\n",
    "    ball_x = random.randint(0, NUM_BALL_X - 1)\n",
    "    ball_y = random.randint(0, NUM_BALL_Y - 1)\n",
    "    ball_dx = random.choice([-1, 1])\n",
    "    ball_dy = random.choice([-1, 1])\n",
    "    \n",
    "    done = False\n",
    "    action_count = 0\n",
    "\n",
    "    while not done and action_count < MAX_ACTIONS_PER_EPISODE:\n",
    "        action_count += 1\n",
    "        state = (paddle_y, ball_x, ball_y, ball_dx, ball_dy)\n",
    "        action = choose_action(state, EPSILON)\n",
    "\n",
    "        paddle_y = max(0, min(NUM_PADDLE_POS - 1, paddle_y + action))\n",
    "\n",
    "        ball_x += ball_dx\n",
    "        ball_y += ball_dy\n",
    "\n",
    "        if ball_y <= 0 or ball_y >= NUM_BALL_Y - 1:\n",
    "            ball_dy = -ball_dy\n",
    "        if ball_x <= 0:\n",
    "            ball_dx = -ball_dx\n",
    "        if ball_x >= NUM_BALL_X - 1:\n",
    "            ball_dx = -ball_dx\n",
    "\n",
    "        next_state = (paddle_y, ball_x, ball_y, ball_dx, ball_dy)\n",
    "\n",
    "        reward = calculate_reward(state, action)\n",
    "\n",
    "        update_q_table(state, action, reward, next_state)\n",
    "\n",
    "    if (episode + 1) % 25000 == 0:\n",
    "        print(f\"Episode {episode + 1}/{EPISODES}\")\n",
    "\n",
    "policy = {str(state): ACTIONS[np.argmax(actions)] for state, actions in q_table.items()}\n",
    "with open(\"pong_q_policy\", \"w\") as f:\n",
    "    json.dump(policy, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5a8283-db75-4adf-9858-8c6b711f54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_file = \"pong_q_policy.json\"\n",
    "game = PongGame(policy_file)\n",
    "game.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
