{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baae6db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Pong import PongGame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98fb1c",
   "metadata": {},
   "source": [
    "# Linear Approximation\n",
    "\n",
    "Linear function approximation is a method used in reinforcement learning to estimate the Q-values when the state-action space is large or continuous. Instead of maintaining a large Q-table, we approximate the Q-function using a linear model.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Model Structure:**  \n",
    "  The Q-function is approximated by a simple linear layer without any hidden layers or nonlinear activation functions. The model takes the state representation as input and outputs Q-values for each possible action.\n",
    "\n",
    "- **Input and Output:**  \n",
    "  - Input: Feature vector representing the current state (e.g., paddle position, ball position and direction).  \n",
    "  - Output: Vector of Q-values corresponding to each possible action.\n",
    "\n",
    "- **Advantages:**  \n",
    "  - Computationally efficient and fast to train.  \n",
    "  - Simpler and less prone to overfitting compared to deep networks.  \n",
    "  - Suitable for problems where the relationship between state and Q-values is approximately linear.\n",
    "\n",
    "- **Limitations:**  \n",
    "  - Cannot capture complex, nonlinear relationships in the data.  \n",
    "  - Performance may degrade on more complicated environments where nonlinear function approximators (e.g., deep neural networks) excel.\n",
    "\n",
    "## How it Works in Practice\n",
    "\n",
    "- The linear model predicts Q-values using:  \n",
    "  $$Q(s, a) \\approx \\mathbf{w}_a^\\top \\mathbf{x}_s + b_a$$\n",
    "  where $\\mathbf{x}_s$ is the feature vector of state $s$, and $\\mathbf{w}_a, b_a$ are the weights and bias for action $a$.\n",
    "\n",
    "- During training, we minimize the mean squared error between predicted Q-values and target Q-values derived from the Bellman equation:  \n",
    "  $$\\text{Loss} = \\left(Q(s, a) - \\left(r + \\gamma \\max_{a'} Q(s', a')\\right)\\right)^2$$\n",
    "\n",
    "- The parameters $\\mathbf{w}_a$ and $b_a$ are updated using gradient descent methods such as Adam optimizer.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Using a linear function approximator for Q-learning provides a simple yet effective way to generalize across states without storing an explicit Q-table. It is best suited for environments where state-action values can be reasonably approximated by linear functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f23b598-06c8-4f5a-822b-3fe593cf233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "EPSILON = 0.1\n",
    "EPISODES = 2500\n",
    "MAX_ACTIONS_PER_EPISODE = 1000\n",
    "\n",
    "NUM_PADDLE_POS = 30\n",
    "NUM_BALL_X = 50\n",
    "NUM_BALL_Y = 40\n",
    "\n",
    "ACTIONS = [-1, 0, 1]\n",
    "\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "PADDLE_WIDTH, PADDLE_HEIGHT = 10, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3c1d69-5610-491e-a464-12acae924350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(state_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "state_dim = 5  # paddle_y, ball_x, ball_y, ball_dx, ball_dy\n",
    "action_dim = len(ACTIONS)\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307dab59-ef4e-4b68-a7df-78843ca224f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(value, max_value, bins):\n",
    "    return min(bins - 1, max(0, int(value / max_value * bins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c914e6d-4365-4970-acac-af2baa714aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_features(state):\n",
    "    paddle_y, ball_x, ball_y, ball_dx, ball_dy = state\n",
    "    return torch.tensor([\n",
    "        paddle_y / NUM_PADDLE_POS,\n",
    "        ball_x / NUM_BALL_X,\n",
    "        ball_y / NUM_BALL_Y,\n",
    "        ball_dx,\n",
    "        ball_dy\n",
    "    ], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9329c5-3f97-4791-b868-34854820ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        features = state_to_features(state)\n",
    "        q_values = q_network(features)\n",
    "        return ACTIONS[torch.argmax(q_values).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a91c36-8940-4623-af49-c00a24f9d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(state, action):\n",
    "    paddle_y, ball_x, ball_y, ball_dx, ball_dy = state\n",
    "\n",
    "    paddle_center = paddle_y * (HEIGHT / NUM_PADDLE_POS) + PADDLE_HEIGHT / 2\n",
    "    ball_actual_y = ball_y * (HEIGHT / NUM_BALL_Y)\n",
    "\n",
    "    distance = abs(paddle_center - ball_actual_y)\n",
    "    reward = -distance / (HEIGHT / 2)\n",
    "\n",
    "    if (ball_actual_y > paddle_center and action == 1) or (ball_actual_y < paddle_center and action == -1):\n",
    "        reward += 0.5\n",
    "\n",
    "    if ball_dx == 1 and ball_x == NUM_BALL_X - 1:\n",
    "        if distance <= PADDLE_HEIGHT / 2:\n",
    "            reward += 5\n",
    "        else:\n",
    "            reward -= 100\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140972f6-7fae-4c60-83ae-19487d59e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_network(state, action, reward, next_state, done):\n",
    "    features = state_to_features(state)\n",
    "    next_features = state_to_features(next_state)\n",
    "\n",
    "    q_values = q_network(features)\n",
    "    next_q_values = q_network(next_features)\n",
    "\n",
    "    action_index = ACTIONS.index(action)\n",
    "    target = reward\n",
    "    if not done:\n",
    "        target += DISCOUNT_FACTOR * torch.max(next_q_values).item()\n",
    "\n",
    "    target_q_values = q_values.clone().detach()\n",
    "    target_q_values[action_index] = target\n",
    "\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aa83675-5075-4579-a553-a360046c712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/2500\n",
      "Episode 200/2500\n",
      "Episode 300/2500\n",
      "Episode 400/2500\n",
      "Episode 500/2500\n",
      "Episode 600/2500\n",
      "Episode 700/2500\n",
      "Episode 800/2500\n",
      "Episode 900/2500\n",
      "Episode 1000/2500\n",
      "Episode 1100/2500\n",
      "Episode 1200/2500\n",
      "Episode 1300/2500\n",
      "Episode 1400/2500\n",
      "Episode 1500/2500\n",
      "Episode 1600/2500\n",
      "Episode 1700/2500\n",
      "Episode 1800/2500\n",
      "Episode 1900/2500\n",
      "Episode 2000/2500\n",
      "Episode 2100/2500\n",
      "Episode 2200/2500\n",
      "Episode 2300/2500\n",
      "Episode 2400/2500\n",
      "Episode 2500/2500\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES):\n",
    "    paddle_y = random.randint(0, NUM_PADDLE_POS - 1)\n",
    "    ball_x = random.randint(0, NUM_BALL_X - 1)\n",
    "    ball_y = random.randint(0, NUM_BALL_Y - 1)\n",
    "    ball_dx = random.choice([-1, 1])\n",
    "    ball_dy = random.choice([-1, 1])\n",
    "\n",
    "    done = False\n",
    "    action_count = 0\n",
    "\n",
    "    while not done and action_count < MAX_ACTIONS_PER_EPISODE:\n",
    "        action_count += 1\n",
    "        state = (paddle_y, ball_x, ball_y, ball_dx, ball_dy)\n",
    "\n",
    "        action = choose_action(state, EPSILON)\n",
    "        paddle_y = max(0, min(NUM_PADDLE_POS - 1, paddle_y + action))\n",
    "\n",
    "        ball_x += ball_dx\n",
    "        ball_y += ball_dy\n",
    "\n",
    "        if ball_y <= 0 or ball_y >= NUM_BALL_Y - 1:\n",
    "            ball_dy = -ball_dy\n",
    "        if ball_x <= 0:\n",
    "            ball_dx = -ball_dx\n",
    "        if ball_x >= NUM_BALL_X - 1:\n",
    "            ball_dx = -ball_dx\n",
    "\n",
    "        next_state = (paddle_y, ball_x, ball_y, ball_dx, ball_dy)\n",
    "\n",
    "        reward = calculate_reward(state, action)\n",
    "        done = ball_x == 0 or ball_x == NUM_BALL_X - 1\n",
    "\n",
    "        update_q_network(state, action, reward, next_state, done)\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode + 1}/{EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b79bea03-ccd5-4841-ac1b-b6c4a59ce5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_possible_states():\n",
    "    states = []\n",
    "    for paddle_y in range(NUM_PADDLE_POS):\n",
    "        for ball_x in range(NUM_BALL_X):\n",
    "            for ball_y in range(NUM_BALL_Y):\n",
    "                for ball_dx in [-1, 1]:\n",
    "                    for ball_dy in [-1, 1]:\n",
    "                        states.append((paddle_y, ball_x, ball_y, ball_dx, ball_dy))\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ee3ab8a-a704-4737-93b0-617b66a29b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = generate_all_possible_states()\n",
    "\n",
    "policy = {\n",
    "    str(state): ACTIONS[\n",
    "        torch.argmax(q_network(state_to_features(state)).detach()).item()\n",
    "    ]\n",
    "    for state in states\n",
    "}\n",
    "\n",
    "with open(\"pong_la.json\", \"w\") as f:\n",
    "    json.dump(policy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a3d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_file = \"pong_la.json\"\n",
    "game = PongGame(policy_file)\n",
    "game.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
